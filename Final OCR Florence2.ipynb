{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12606021,"sourceType":"datasetVersion","datasetId":7962574}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install \"numpy<2.0\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python --version\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Uninstall libraries to ensure a clean environment\n!pip uninstall -y torch torchvision torchaudio transformers accelerate bitsandbytes peft\n!pip uninstall flash-attn -y\n\n# Install compatible libraries for your GPU\n!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n!pip install transformers==4.41.2 accelerate==0.30.1 bitsandbytes==0.43.1 peft==0.11.1\n\nprint(\"âœ… Installation complete. Please restart the runtime now.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nfrom PIL import Image\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoProcessor, BitsAndBytesConfig, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T12:18:46.025716Z","iopub.execute_input":"2025-07-31T12:18:46.026350Z","iopub.status.idle":"2025-07-31T12:18:46.031209Z","shell.execute_reply.started":"2025-07-31T12:18:46.026329Z","shell.execute_reply":"2025-07-31T12:18:46.030427Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport transformers\nimport bitsandbytes\nimport peft\nimport accelerate\n\nprint(\"--- Library Versions ---\")\nprint(f\"Torch: {torch.__version__}\")\nprint(f\"Transformers: {transformers.__version__}\")\nprint(f\"Bitsandbytes: {bitsandbytes.__version__}\")\nprint(f\"PEFT: {peft.__version__}\")\nprint(f\"Accelerate: {accelerate.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T12:18:49.667070Z","iopub.execute_input":"2025-07-31T12:18:49.667757Z","iopub.status.idle":"2025-07-31T12:18:49.800798Z","shell.execute_reply.started":"2025-07-31T12:18:49.667730Z","shell.execute_reply":"2025-07-31T12:18:49.799717Z"}},"outputs":[{"name":"stdout","text":"--- Library Versions ---\nTorch: 2.3.0+cu121\nTransformers: 4.41.2\nBitsandbytes: 0.43.1\nPEFT: 0.11.1\nAccelerate: 0.30.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Step 1: Consolidate Image and Annotation Paths into a DataFrame ","metadata":{}},{"cell_type":"code","source":"def create_dataset_df(image_dir, annot_dir):\n    \"\"\"Pairs images with their JSON annotations and returns a pandas DataFrame.\"\"\"\n    image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n    data = []\n    for img_file in image_files:\n        base_name = os.path.splitext(img_file)[0]\n        annot_file = base_name + '.json'\n        image_path = os.path.join(image_dir, img_file)\n        annot_path = os.path.join(annot_dir, annot_file)\n        if os.path.exists(annot_path):\n            data.append({'image_path': image_path, 'json_path': annot_path})\n    return pd.DataFrame(data)\n\n# Define file paths\nbase_dir = '/kaggle/input/ocr-dataset-for-healthcare/dataset'\ntrain_images_dir = os.path.join(base_dir, 'training_data', 'images')\ntrain_annots_dir = os.path.join(base_dir, 'training_data', 'annotations')\ntest_images_dir = os.path.join(base_dir, 'testing_data', 'images')\ntest_annots_dir = os.path.join(base_dir, 'testing_data', 'annotations')\n\ntrain_df = create_dataset_df(train_images_dir, train_annots_dir)\ntest_df = create_dataset_df(test_images_dir, test_annots_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T12:18:52.929677Z","iopub.execute_input":"2025-07-31T12:18:52.929977Z","iopub.status.idle":"2025-07-31T12:18:53.594900Z","shell.execute_reply.started":"2025-07-31T12:18:52.929953Z","shell.execute_reply":"2025-07-31T12:18:53.594338Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Step 2: Convert to Hugging Face Dataset Format ","metadata":{}},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\ntrain_val_split = train_dataset.train_test_split(test_size=0.1, seed=42)\n\ndataset_dict = DatasetDict({\n    'train': train_val_split['train'],\n    'validation': train_val_split['test'],\n    'test': test_dataset\n})\n\nprint(\"Created Hugging Face DatasetDict:\")\nprint(dataset_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T12:18:56.678425Z","iopub.execute_input":"2025-07-31T12:18:56.679008Z","iopub.status.idle":"2025-07-31T12:18:56.742899Z","shell.execute_reply.started":"2025-07-31T12:18:56.678986Z","shell.execute_reply":"2025-07-31T12:18:56.742107Z"}},"outputs":[{"name":"stdout","text":"Created Hugging Face DatasetDict:\nDatasetDict({\n    train: Dataset({\n        features: ['image_path', 'json_path'],\n        num_rows: 134\n    })\n    validation: Dataset({\n        features: ['image_path', 'json_path'],\n        num_rows: 15\n    })\n    test: Dataset({\n        features: ['image_path', 'json_path'],\n        num_rows: 50\n    })\n})\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Step 3: Define the Core Processing Function","metadata":{}},{"cell_type":"code","source":"TASK_PROMPT = \"<OD>\"\nTOKENIZER_MAX_COORD = 999\n\ndef process_example(example):\n    \"\"\"\n    Loads image, formats the target text from JSON, and returns None if no valid\n    annotations are found.\n    \"\"\"\n    image = Image.open(example['image_path']).convert(\"RGB\")\n    with open(example['json_path'], 'r') as f:\n        annotations = json.load(f)\n    target_text = \"\"\n    \n    # Check if the 'form' key exists and is a non-empty list\n    if 'form' not in annotations or not isinstance(annotations['form'], list) or not annotations['form']:\n        print(f\"WARNING: No 'form' data in {example.get('json_path', 'unknown file')}. Skipping.\")\n        return None\n\n    for item in annotations.get('form', []):\n        box = item['box']\n        text = item['text']\n        \n        # Skip if text is missing or box is malformed\n        if not text or len(box) != 4:\n            continue\n\n        try:\n            x1 = int(min(max(0, box[0]), TOKENIZER_MAX_COORD))\n            y1 = int(min(max(0, box[1]), TOKENIZER_MAX_COORD))\n            x2 = int(min(max(0, box[2]), TOKENIZER_MAX_COORD))\n            y2 = int(min(max(0, box[3]), TOKENIZER_MAX_COORD))\n        except (ValueError, TypeError):\n            continue\n\n        box_str = f\"<loc_{x1}><loc_{y1}><loc_{x2}><loc_{y2}>\"\n        target_text += box_str + text\n    \n    # --- FIX: If after all processing, the target is empty, return None ---\n    if not target_text:\n        print(f\"WARNING: No valid text/box pairs found in {example.get('json_path', 'unknown file')}. Skipping.\")\n        return None\n    \n    return {'image': image, 'prompt': TASK_PROMPT, 'target_text': target_text}\n\n# --- Map the function and then FILTER out the None values ---\nprocessed_dataset = dataset_dict.map(\n    process_example, \n    remove_columns=['image_path', 'json_path']\n)\n\n# This is the crucial new step:\nfinal_processed_dataset = processed_dataset.filter(lambda example: example is not None)\n\n\nprint(\"\\n--- Dataset Cleaning Report ---\")\nprint(f\"Original training samples: {len(dataset_dict['train'])}\")\nprint(f\"Cleaned training samples: {len(final_processed_dataset['train'])}\")\nprint(f\"Original validation samples: {len(dataset_dict['validation'])}\")\nprint(f\"Cleaned validation samples: {len(final_processed_dataset['validation'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T12:18:59.178786Z","iopub.execute_input":"2025-07-31T12:18:59.179074Z","iopub.status.idle":"2025-07-31T12:19:13.873378Z","shell.execute_reply.started":"2025-07-31T12:18:59.179054Z","shell.execute_reply":"2025-07-31T12:19:13.872729Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/134 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bf001ec7a7f4be5876b6d99d7af10de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/15 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf894afecec74095a8c67fbc21e93a8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0494c5e4651a416f8a934ec51655f80e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/134 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f9313997c38467398e96d519f47420e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/15 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc536c37da0c4727a55aecee99929ec9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d8c4ef2c7e244628efe40461a920316"}},"metadata":{}},{"name":"stdout","text":"\n--- Dataset Cleaning Report ---\nOriginal training samples: 134\nCleaned training samples: 134\nOriginal validation samples: 15\nCleaned validation samples: 15\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Step 4: Define and Apply the Final Florence-2 Processor","metadata":{}},{"cell_type":"code","source":"# # 3. Setup Processor and Process Dataset for the Model\n# model_name = \"microsoft/Florence-2-base\"\n# processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n# if processor.tokenizer.pad_token is None:\n#     processor.tokenizer.pad_token = processor.tokenizer.eos_token\n\n# def apply_processor_batched(batch):\n#     inputs = processor(text=batch[\"prompt\"], images=batch[\"image\"], return_tensors=\"pt\", padding=True, truncation=True)\n#     labels = processor.tokenizer(text=batch[\"target_text\"], return_tensors=\"pt\", padding=True, max_length=2048, truncation=True).input_ids\n#     labels[labels == processor.tokenizer.pad_token_id] = -100\n#     inputs[\"labels\"] = labels\n#     return inputs\n# final_dataset = final_processed_dataset.map(apply_processor_batched, batched=True, remove_columns=list(final_processed_dataset[\"train\"].features))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"\\nDataset is fully processed and ready for training!\")\n# print(final_dataset)\n# print(\"Keys in a single training sample:\", final_dataset['train'][0].keys())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 5: Fine-Tuning the Florence-2 Model","metadata":{}},{"cell_type":"code","source":"# # Define Quantization Configuration (Q-LoRA) \n# # This config tells the model to load its weights in 4-bit precision.\n# quantization_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.float16, # Use float16 for stability\n#     bnb_4bit_use_double_quant=True,\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install flash-attn==2.5.8 --no-build-isolation # Configured according to CUDA version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T12:19:28.147768Z","iopub.execute_input":"2025-07-31T12:19:28.148066Z","iopub.status.idle":"2025-07-31T12:19:41.910862Z","shell.execute_reply.started":"2025-07-31T12:19:28.148046Z","shell.execute_reply":"2025-07-31T12:19:41.910058Z"}},"outputs":[{"name":"stdout","text":"Collecting flash-attn==2.5.8\n  Using cached flash_attn-2.5.8.tar.gz (2.5 MB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.5.8) (2.3.0+cu121)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.5.8) (0.8.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.5.8) (25.0)\nRequirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.5.8) (1.11.1.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (4.14.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (12.1.105)\nRequirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.5.8) (2.3.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn==2.5.8) (12.5.82)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn==2.5.8) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->flash-attn==2.5.8) (1.3.0)\nBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for flash-attn: filename=flash_attn-2.5.8-cp311-cp311-linux_x86_64.whl size=120938665 sha256=0b28549d7a7105dfa9300751de038f1e607fe11bb96ac2bdd29b474926ca8e41\n  Stored in directory: /root/.cache/pip/wheels/2a/88/b2/587b498e2caa887707a63d0ed7d7f4beca27f5034640382845\nSuccessfully built flash-attn\nInstalling collected packages: flash-attn\nSuccessfully installed flash-attn-2.5.8\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, quantization_config=quantization_config)\n# model.resize_token_embeddings(len(processor.tokenizer))\n# model = prepare_model_for_kbit_training(model)\n# lora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", target_modules=\"all-linear\", task_type=\"CAUSAL_LM\")\n# model = get_peft_model(model, lora_config)\n# model.config.use_cache = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # bf16_supported = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n# # FIX: Set fp16=True and bf16=False \n# training_args = TrainingArguments(\n#     output_dir=\"./florence2-qlora-healthcare-ocr\",\n#     num_train_epochs=50,\n#     learning_rate=2e-5,\n#     per_device_train_batch_size=1, \n#     per_device_eval_batch_size=2,\n#     gradient_accumulation_steps=4,\n    \n#     # Use fp16 for T4 stability\n#     fp16=True,\n#     bf16=False,\n    \n#     gradient_checkpointing=False,\n#     logging_steps=25,\n#     eval_strategy=\"epoch\",\n#     save_strategy=\"epoch\",\n#     load_best_model_at_end=True,\n#     metric_for_best_model=\"eval_loss\",\n#     greater_is_better=False,\n#     report_to=\"none\"\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=final_dataset[\"train\"],\n#     eval_dataset=final_dataset[\"validation\"]\n# )\n\n# print(\"Starting model fine-tuning...\")\n# trainer.train()\n# print(\"Fine-tuning complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add AutoModelForCausalLM to this line\nfrom transformers import AutoModelForCausalLM, AutoProcessor, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T12:21:03.082488Z","iopub.execute_input":"2025-07-31T12:21:03.082800Z","iopub.status.idle":"2025-07-31T12:21:03.086867Z","shell.execute_reply.started":"2025-07-31T12:21:03.082778Z","shell.execute_reply":"2025-07-31T12:21:03.086276Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# (Your data processing code should be in the cells above this)\n\n# 1. Setup Processor\nmodel_name = \"microsoft/Florence-2-base\"\nprocessor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\nif processor.tokenizer.pad_token is None:\n    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n\n# 2. Model Configuration with Manual Device Mapping\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n    load_in_8bit=True,\n    device_map={\"\": 0}  # <-- FIX: Manually assign the model to the first GPU\n)\n\n# 3. Resize embeddings and prepare for LoRA\nmodel.resize_token_embeddings(len(processor.tokenizer))\nmodel = prepare_model_for_kbit_training(model)\nlora_config = LoraConfig(\n    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", \n    target_modules=\"all-linear\", task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, lora_config)\nmodel.config.use_cache = False\nmodel.print_trainable_parameters()\n\n# 4. Training Arguments\n# Training arguments for maximum stability\ntraining_args = TrainingArguments(\n    output_dir=\"./florence2-lora-8bit-fp32-ocr\", # New output directory\n    num_train_epochs=50,\n    learning_rate=2e-5,\n    per_device_train_batch_size=1, \n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    \n    # --- THE FINAL FIX ---\n    fp16=False, # Disable mixed-precision training\n    bf16=False,\n                                 \n    gradient_checkpointing=True,\n    logging_steps=25,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)\n\n# 5. Initialize and Run Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=final_dataset[\"train\"],\n    eval_dataset=final_dataset[\"validation\"]\n)\n\nprint(\"\\nStarting model fine-tuning with STABLE 8-BIT configuration...\")\ntrainer.train()\nprint(\"\\nFine-tuning complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T12:27:38.498246Z","iopub.execute_input":"2025-07-31T12:27:38.498716Z","iopub.status.idle":"2025-07-31T12:27:58.247400Z","shell.execute_reply.started":"2025-07-31T12:27:38.498690Z","shell.execute_reply":"2025-07-31T12:27:58.246336Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 7,319,232 || all params: 238,733,248 || trainable%: 3.0659\n\nStarting model fine-tuning with STABLE 8-BIT configuration...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_296/2935104085.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting model fine-tuning with STABLE 8-BIT configuration...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFine-tuning complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1885\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1886\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2260\u001b[0m                             )\n\u001b[1;32m   2261\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2262\u001b[0;31m                             _grad_norm = self.accelerator.clip_grad_norm_(\n\u001b[0m\u001b[1;32m   2263\u001b[0m                                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2264\u001b[0m                                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2267\u001b[0m                     \u001b[0;31m# Set is_xla_gradients_synced to True to avoid all-reduce twice in the AcceleratedOptimizer step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2268\u001b[0m                     \u001b[0macc_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_xla_gradients_synced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36munscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAcceleratedOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2219\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36munscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mfound_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n\u001b[0m\u001b[1;32m    338\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_inf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    257\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mallow_fp16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempting to unscale FP16 gradients.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                         \u001b[0;31m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."],"ename":"ValueError","evalue":"Attempting to unscale FP16 gradients.","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"# After training, save your fine-tuned model and the processor to a directory.\n# This allows you to easily load it later for inference.\n\nfinal_model_dir = \"./florence2-healthcare-ocr-final\"\nprint(f\"\\nSaving the best model to {final_model_dir}\")\ntrainer.save_model(final_model_dir)\nprocessor.save_pretrained(final_model_dir) # The processor was defined in the previous step\n\nprint(\"\\n--- Your model is fine-tuned and saved! ---\")\nprint(\"You can now use it for inference on new documents.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}